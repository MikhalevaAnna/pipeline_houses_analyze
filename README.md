# Проанализировать информацию о домах

Есть csv файл, расположенный по ссылке. Скачивайте по ссылке: https://disk.yandex.ru/d/PBWn9vwv_xSk8A <br>
Размер распакованного файла около 300 Мб. Количество строк - 590708 строк.

## Данные в файле:
**house_id** - `ID объекта` - (например, 590707) <br>
**latitude**, **longitude** - `Координаты` - (Широта, Долгота) (например, 55.060199, 32.695577) <br>
**maintenance_year** - `Год постройки` - (например, 1953.0) <br>
**square** - `Площадь` - (например, 585.60) <br>
**population** - `Количество этажей` - (например, 18) <br>
**region** - `Область`  - (например, Смоленская область) <br> 
**locality_name** - `Город` - (например, Ярцево) <br>
**address** - `Адрес` (например, "ул. Братьев Шаршановых, д. 61") <br>
**full_address** - `Полный адрес` (например,  "Ставропольский край, р-н. Александровский, с. Александровское, ул. Калинина, д. 25") <br>
**communal_service_id** - `ID ЖКУ` <br>
**description** - `Описание объекта` - (например, "Жилой дом в Ярцево, по адресу ул. Братьев Шаршановых, д. 61, 1953 года постройки, под управлением ТСЖ «Шаршановых».") <br>

## Выполнение задания:

1. Загружаем файл данных в **DataFrame PySpark**. Обязательно выводим количество строк. <br>

2. Убедимся, что данные корректно прочитаны (правильный формат, отсутствие пустых строк). <br>

3. Преобразуем текстовые и числовые поля в соответствующие типы данных (например, дата, число). <br>
 
4. Вычислим средний и медианный год постройки зданий. <br>

5. Определим топ-10 областей и городов с наибольшим количеством объектов. <br>

6. Найдем здания с максимальной и минимальной площадью в рамках каждой области. <br>

7. Определим количество зданий по десятилетиям (например, сколько зданий построено в 1950-х, 1960-х и т.д.). <br>

## С данными поработали, проанализировали, теперь можно их и в БД положить. 

8. Создадим схему таблицы в **ClickHouse**, которая будет соответствовать структуре наших данных в **airflow**. <br>

9. Настроим соединение с **ClickHouse** из скрипта в **airflow**. <br>

10. Загрузим обработанные данные из **DataFrame** в таблицу в **ClickHouse** в **airflow**. <br>

11. Выполним **SQL** скрипт в **Python**, который выведет топ 25 домов, у которых площадь больше 60 кв.м в **airflow**. <br>

Все операции по созданию таблиц, обработке, миграции данных происходят внутри **Airflow**.


### Просмотр данных через DBeaver:
В **DBeaver**, после успешной работы пайплайна, можно посмотреть полученные таблицы `houses_data` в **ClickHouse**.


#### Для подключение к **ClickHouse** используются следующие параметры:
```
    Хост: localhost
    Порт: 8123
```
#### Команды для запуска проекта:
```bash
    git clone https://github.com/MikhalevaAnna/pipeline_houses_analyze.git
    cd pipeline_houses_analyze
    docker build -t airflow-with-java .
    docker-compose up --build
```
    
- Далее идем по адресу - **http://localhost:8080**
- Логин и пароль - **airflow**.
